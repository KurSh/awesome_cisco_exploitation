/* $Id: C7200_Predator,v 3.1.2.2 1996/07/15 15:32:50 mbeesley Exp $
 * $Source: /release/112/cvs/Xsys/Doc/C7200_Predator,v $
 *------------------------------------------------------------------
 * C7200_Predator : Documentation/FAQ for C7200 (aka Predator)
 *
 * April 1996, Michael Beesley
 *
 * Copyright (c) 1996 by cisco Systems, Inc.
 * All rights reserved.
 *------------------------------------------------------------------
 * $Log: C7200_Predator,v $
 * Revision 3.1.2.2  1996/07/15  15:32:50  mbeesley
 * CSCdi62569:  Update C7200 doc
 * Branch: California_branch
 *
 * Revision 3.1.2.1  1996/04/30  18:26:40  mbeesley
 * CSCdi56203:  Add some documentation
 * Branch: California_branch
 *
 * Revision 3.1  1996/04/30  18:24:16  mbeesley
 * Add placeholder
 *
 *------------------------------------------------------------------
 * $Endlog$
 */

[0] Contents
============

	[1]  Predator System Introduction
	[2]  Software / Product Naming
	[3]  Predator System Details
	[4]  Predator Software Architecture
	[5]  Predator Packet Format
	[6]  Predator Features, Pros and Cons
	[7]  Predator Release Vehicle
	[8]  Predator IDB types
	[9]  Supported interfaces and future interfaces
	[10] Performance and Data Cache usage
	[11] Buffers
	[12] Predator Memory Map

[1] Predator System Introduction
================================

	The C7200 is a modular router. It presently has 6 hot-swappable
	PCI port adaptor slots which can carry media cards as well as non-media
	accelerator cards (encryption engine, compression engine etc).

	It is an R4600 based single CPU router. The C7206 has a 150 Mhz
	CPU along with a 512kbyte L2 cache. The external bus frequency is
	50 Mhz and the PCI bus frequency is 25 Mhz.

[2] Software / Product Naming
=============================

	The Predator platform started its life as the C7100 series router.
	After long and deliberate discussions with Marketing, C7100 was chosen
	as the product name. Engineering then proceeded to design and write the
	IOS source code to support this. The directory structure and filename
	convention was chosen based on C7100. However, at the end of the 
	development cycle, marketing received some input from Europe that 
	indicated that C7100 was not a good name and the product needed to be
	called the C7200. This was way too late to change the directory and
	filename structure. So engineering has changed only the externally
	visible parts of the code to reflect the new C7200 name. So the
	source code is still centered arround c7100. IE

		sys/src-4k-c7100	- Predator source
		sys/obj-4k-c7100	- Predator object directory
		sys/rommon/obj-4k-c7100	- Predator ROM Monitor
		CPU_C7100		- Predator cpu_type

	The executable filenames have been changed to c7200 along with all
	SNMP MIB variables and all command line commands and responses. 

	So keep in mind that all images and UI/SNMP stuff is C7200 while
	all directory and source naming is C7100 !!!

[3] Predator System Details
===========================

	The Predator system is quite complex from a memory/bus architectural
	point of view, although most of this should be hidden from the vast
	majority of code development effort. The system is a PCI bus based
	system with six OIR'able port adaptors along with a fast ethernet 
	port on the system IO card. Mechanically the box looks a bit like :

	Side View :
	
				 ---
		PA 5,6	--------| | |  --------------
				| | | |              |
				| | | |              | Dual Power Supply
		PA 3,4	--------| | | |		     |
				| | | |              |
				| | |  --------------
		PA 1,2	--------| | |
				| | |
				| | |
		IO Card --------|-|-|----------------- CPU Card
		(fastether 0/0)


	The IO Card, Midplane and CPU cad are all field replacable units. The
	cards have the following resources :

	IO Card :	1 Fast ethernet port
			Console/Aux port
			Bootflash simm
			Dual PCMCIA slots
			Boot ROM
			NVRAM
			IO register file (environment monitor, reset etc)

	Midplane :	Six port adaptor slots
			Six PCI/PCI bridge chips for bus isolation (load, OIR)
			Register file to control port adaptors
			
	CPU Card :	150Mhz R4700 CPU
			512Kbyte L2 cache
			DRAM simms (4 simm sockets)
			1 Mbyte fast packet SRAM
			4 PCI/PCI bridge chips
			GT64010 r4700/PCI bridge chip with
				4 DMA engines and 4 timer/counters.

	From a memory/bus architecture, the box looks as follows :


			 -------	     --------
			| 	|	    |L2 Cache|
			| R4700 |	     --------
			|	|		|
			 -------		|
			    |			|
      ----------------------+-------------------
     |			    |
     |			    |-------------------
     |			    |			|
  -------		 -------		|	 -------
 |       |		| 	|		|	|	|
 | SRAM  |		|GT64010|---------------+-------| DRAM	|
 |       |		|	|		|	|	|
  -------		 -------		|	 -------
   |  |			    |			|
   |  |		PCMCIA	    |			|
   |  |		 -----------|			 ---------------
   |  |		IO FE	    |				IO Bus
   |  |			    |				======
   |  |		 -----------------------		Boot ROM
   |  |		|   |		    |   |		NVRAM
   |  |		B   B		    B   B		Register file
   |  |		|   |		    |   |		UART
   |  |		|---		    |---		Bootflash simm
   |  |		|		    |
   |  |	 PA1 -B-| 		    |-B- PA2
   |  |		|		    |
   |  |		|		    |
   |  |	 PA3 -B-| 		    |-B- PA4
   |  |		|		    |
   |  |		|		    |
   |  |	 PA5 -B-| 		    |-B- PA6
   |  |		|		    |
   |  |		|		    |
   |  |		|		    |
   |   ---------		    |
   |				    |
    --------------------------------			-B- = PCI/PCI Bridge


	As can been seen from the above diagram, all resources in the box are
	available to all port adaptors and to the IO cards PCI devices (PCMCIA
	flash card controller and fastethernet controller). This allows packets
	to be recieved into and transmitted from any memory resource, be it
	packet SRAM or the portion of DRAM that is reserved for packets (which
	shows up as the IO memory in a show memory summary). It also means that
	if at a future date an SSE style port adaptor is developed, it has
	access to the register set on all port adaptors, as well as access to
	packet memory in the box.

[4] Predator Software Architecture
==================================

	- Basically a LES box

	- Device Drivers receive packets as scattered IE in particles

	- Device Driver will never hold onto a paktype header if the
	  packet is scattered. If the packet needs to go onto the
	  output hold queue, a new paktype header will be allocated 
	  and the packet will be queued using the new header. This allows
	  the recieve side of all device drivers to have one static
	  fast switching paktype (allocated from a special buffer pool
	  which has the pool_ret_item vector overriden so that a 
	  datagram_done() can be performed on the device drivers static
	  paktype header and all particles on that paktype will be returned
	  to their pools but the header itself will not, which is just as well
	  as the recieving device driver will use this same paktype for the
	  next packet received !).

	- Supports some scattered based features - IP multicast, IP f/s
	  IPX f/s, atalk f/s and vines f/s, IP flow/optimum f/s

	- Otherwise its vanilla IOS.

[5] Predator Packet Format
==========================

	Predator is a scatter/gather box. This means that packets come into
	the box in scattered discontigous chunks of memory. This has some
	impacts on the software, both positive and negative. On the positive
	side, packet cloning for applications such as IP multicasting,
	bridge flooding, UDP turbo flooding can be done very quickly as there
	is no need to replicate the data in the packet IE no packet bcopy.
	All that needs to happen is that the controlling data structures
	(which are small and live in memory cached by the L2 cache and the
	internal data cache) need to be replicated. The downside is that not
	all interrupt level protocols/features understand how to manipulate
	a scattered packet at this point (IP, IPX, Atalk and Vines all do).
	Also, no process level functionality knows how to deal with a scattered
	packet, so if the incoming packet needs to go to process level,
	a traditional contigous paktype buffer needs to be allocated and
	the packet needs to be coalesced from its particles into the contigous
	buffer. Predator uses one of the 4 DMA engines built into the GT64010
	to do the actual copying of data required to do this, thus preserving
	process level performance (box can do ~ 7kpps, about equivalent to
	a c4500).

	Within each device driver, a check is made to see what protocol
	the packet is. If it is a protocol that can be fastswitched as is,
	the fastswitching routine is called. Otherwise, the packet is sent
	to the DMA device driver to be coalesced, and then fastswitched
	or sent to process level.

	A contigous packet :

	 ---------	 -----------------------------------------------
	| paktype |---->|						|
	 ---------	 -------^---------------------------------------
		|		|
		 ---------------	datagramstart, network_start etc

	A scattered packet :


	 ---------		 
	| paktype |-------------	datagramstart, network_start etc
	 ---------		|
	  |	|		|
	  |     |		*
	  |    ---	   ---------------------
List of	  |   |   |------>|	|ARPA|		|	particle 1
Particle  |    ---	   ---------------------
Headers	  |     |		^
	  |     |
	  |    ---	   ---------------------
	  |   |   |------>|			|	particle 2
	  |    ---	   ---------------------
	  |     |	   ^
	  |     |
	  |    ---	   ---------------------
	   -->|   |------>|			|	particle 3
	       ---	   ---------------------
			   ^

	Note : Each particle header has a pointer to the start of the
	packet data in each particle (^) and a field indicating how
	many bytes of packet data are in each particle.

	So, if a packet is represented in a scattered format using particles,
	and say it was an ethernet frame that needed to be IP multicasted
	out 10 different interfaces if can be accomplished without 
	copying the datagram. First thing that needs to be done is allocate
	one new paktype header (only one is needed as it will not get
	"consumed" by the fastsend vector of the outbound interface). Then
	for each interface :

		- clone the original packet onto the new paktype header
		- snip off the inbound encapsulation
		- allocate a rewrite particle (allocate_fs_particle())
		- enqueue rewrite particle at front of packet
		- write outbound encap into rewrite particle
		- adjust all packet/particle pointers
		- fastsend the packet

	So, a cloned packet with a rewrite on it would look like :

	 ---------		 
	| cloned  |
	| paktype |-------------	datagramstart, network_start etc
	 ---------		|
	  |	|		*
	  |    ---	   ---------------------
	  |   |CP |------>|	|Out Encap|	|	rewrite particle 
	  |    ---	   ---------------------
	  |     |		^
	  |     |		
	  |    ---	   ---------------------
List of	  |   |CP |------>|	|ARPA|		|	particle 1
Particle  |    ---	   ---------------------
Headers	  |     |		     ^
	  |     |
	  |    ---	   ---------------------
	  |   |CP |------>|			|	particle 2
	  |    ---	   ---------------------
	  |     |	   ^
	  |     |
	  |    ---	   ---------------------
	   -->|CP |------>|			|	particle 3
	       ---	   ---------------------
			   ^

	A particle header is 32 bytes, in memory cached by the internal
	data cache as well as the L2 cache. So copying it is very fast, 
	at least an order of magnitude faster than copying the actual data
	bytes from an original inbound frame to a new buffer. Also, the time
	to allocate a cloning paktype header is zero as it only happens on
	the first packet. It is kept forever by the system, and re-used
	over and over. A packet can be cloned without copying any of the
	packet data bytes, and it can be done without disturbing the original
	inbound frame IE if for some reason, the frame cannot be multicasted
	out all interfaces at interrupt level, the original frame is still
	available to send to process level for completion.

	Presently, the only feature that takes advantage of this is IP 
	multicasting. Very soon now, the RSP UDP Turbo flooding implementation
	will be ported to Predator and this cloning mechanism will be used.
	And hopefully, some day, the transparent bridging code can be modified
	to use scattered packets and cloning on Predator for very high speed
	flooding.

[6] Predator Features, Pros and Cons
====================================

	Significant Predator Features :
	
		- OIR of Port Adaptors
		- ISL/VLAN Support		(1)
		- Flow Switching for IP		(1)
		- Optimum Switching for IP	(1)

	What Predator is Good At :

		- Optimum/Flow switching of IP
		  (with access lists)
		- IP Multicast
		- WAN aggregation
		- IBM Feature set
		- Compression			(2)
		- Encryption			(2)
		- Bridging			(3)
		- UDP Turbo Flooding		(4)

	What Predator is Bad At :

		- Very large bandwidth applications (600mbps and above)
		- Very high packet rates (150kpps and above)

	Notes :

	(1) Coming to the feature set real soon
	(2) Coming with the hardware accelerator port adaptors
	(3) Needs to be rewritten to take advantage of Predators packet
	    cloning capabilities
	(4) Needs to be ported from the RSP
	
[7] Predator Release Vehicle
============================

	Predator will be release in a CORE BU special release off of Arkansas
	(versioned as 11.1(472)). It is in the California release
	(11.2) and will be rolled back into Arkansas probably in the 11.1(5)
	release.

[8] Predator IDB types
======================

	Predator has introduce some new IDB types for some interfaces.
	It has also re-used some existing IDB types that are on other 
	platforms. These IDB types are now on Predator.

	New IDB types :

		- IDBTYPE_AMDP2		10mbit ethernet interface
		- IDBTYPE_S4T68360	Fast serial interface
		- IDBTYPE_IBM2692	Token ring interface

	Re-used IDB types :

		- IDBTYPE_FEIP		Fastethernet interface
		- IDBTYPE_FDDIT		FDDI interface

	The re-used IDB types were previously only used on the c7000/c7500.
	The fastethernet IDB type has also been re-used for the fastetherent
	interface on the c4500 product line. Please keep this in mind.

[9] Supported interfaces and future interfaces
==============================================

	At FCS the following port adaptors are supported :

		- 4 port 10mbit ethernet, 10baseT
		- 5 port 10mbit ethernet, fiber
		- 8 port 10mbit etherent, 10baseT
		- 100mbit fastethernet, MII, fx and tx
		- 4 port 4/16mbit token ring
		- 4 port sync serial
		- fddi (sm, mm)

	Post FCS the following port adaptors are planned. These
	port adaptors are in bringup in the lab :

		- 8 port MBRI
		- 1/2 port PRI
		- 8 port sync serial
		- ATM
		- 2 port HSSI
		- Compression engine
		- Encryption engine
		- POSIP 			** May not go on Predator
		- CT3 				** May not go on Predator

	And the following set of port adaptors are in the early design stage :

		- ATM WAN mux
		- Ethernet switching blade (8-12port)
		- Escon channel
		- Bus and Tag

[10] Performance and Data Cache usage
=====================================

	The following gives an outline of the types of performance the predator
	system is capable of. This information is not meant to be official
	performance results. Rather it is intended to give developers a 
	ball park feeling for what the box can presently do. This should help 
	get people familiar to the box and to somewhat set reasonable
	expectations. The numbers are for typical configurations. They change
	as interface type/mix and number change.

		Normal IP fastswitching		: 140kpps
		IP Optimum switching		: 145kpps
		IP Flow switching		: 120kpps
		IP Process switching		: 7kpps
		ISL routing/translation		: 60kpps

	Note : The number above were measured without any use of the internal
	data cache of the r4700 CPU for packet data. The code has been designed
	and written to use the data cache for IP flow and optimum switching.
	However, in an effort to have the highest quality possible for the
	first release, this has been defeated. After an appropriate amount of 
	testing, the data cache will be enabled for flow/optimum switching.
	Apart from these two switching paths, the data cache is never used
	for packet data.

[11] Buffers
============

	The output of "show buffers" on a Predator system is somewhat more
	complex then on other boxes. This is mainly due to the fact that
	the Predator system is a hybrid between the c7500 and the c4700. The
	box has a 1 mbyte chunk of SRAM which is analgous to MEMD on the
	c7500, but is a completely IOS managed block of memory so details
	of it show up on "show buffers". There is also packet DRAM which
	is consistent with the c4700 and also shows up on "show buffers".

	Below is an annotated copy of a typical "show buffers" output.


* pred-ios#sh buffers
* Buffer elements:
*     500 in free list (500 max allowed)
*     1350 hits, 0 misses, 0 created

	This is the element free list. Apart from other things, when 
	a packet goes on an output hold queue, an element is required.

*Public buffer pools:
*Small buffers, 104 bytes (total 53, permanent 50):
*     46 in free list (20 min, 150 max allowed)
*     1087 hits, 1 misses, 0 trims, 3 created
*     0 failures (0 no memory)
*Middle buffers, 600 bytes (total 25, permanent 25):
*     25 in free list (10 min, 150 max allowed)
*     208 hits, 0 misses, 0 trims, 0 created
*     0 failures (0 no memory)
*Big buffers, 1524 bytes (total 50, permanent 50):
*     48 in free list (5 min, 150 max allowed)
*     39 hits, 0 misses, 0 trims, 0 created
*     0 failures (0 no memory)
*VeryBig buffers, 4520 bytes (total 10, permanent 10):
*     10 in free list (0 min, 100 max allowed)
*     0 hits, 0 misses, 0 trims, 0 created
*     0 failures (0 no memory)
*Large buffers, 5024 bytes (total 0, permanent 0):
*     0 in free list (0 min, 10 max allowed)
*     0 hits, 0 misses, 0 trims, 0 created
*     0 failures (0 no memory)
*Huge buffers, 18024 bytes (total 0, permanent 0):
*     0 in free list (0 min, 4 max allowed)
*     0 hits, 0 misses, 0 trims, 0 created
*     0 failures (0 no memory)
 
	These are the traditional buffer pools. They are configurable 
	by the user as per other platforms. When recieved packets are
	destined for process level, the data is copied from the recieve
	interfaces particles to one of these buffers. This is either done
	by the CPU or done by the DMA engines.

*Header pools:
*Header buffers, 0 bytes (total 511, permanent 256):
*     255 in free list (256 min, 1024 max allowed)
*     171 hits, 85 misses, 0 trims, 255 created
*     0 failures (0 no memory)
*     256 max cache size, 256 in cache

	When a packet is recieved and fastswitched in scatter gather format
	IE the data is in particles rather then contigous buffers, and there
	is not enough room on the output interfaces transmit ring, the 
	packet is moved to the output hold queue. To go there, a new paktype 
	header is required, as the existing paktype header is statically
	allocated to the recieve interface (and will be re-used for the
	next packet). So, a new paktype header is allocated from this pool.

*Particle Clones:
*     1024 clones, 108 hits, 0 misses

	For flooding applications, (IP multicasting is the only presently
	implemented example, but UDP turbo flooding and bridge flooding
	are on the way), a scatter/gather packet can be flood out many
	interfaces simultaneously without copying the packet data IE the data
	in the particles, for each outbound interface. However the particle
	management data structure (particletype) needs to be copied for
	each particle going out each interface. To this end, a queue of
	particletype clones is available. This line shows the number of
	free particle clones, the hit count and the miss count.

*Public particle pools:
*F/S buffers, 128 bytes (total 512, permanent 512):
*     0 in free list (0 min, 512 max allowed)
*     512 hits, 0 misses
*     512 max cache size, 512 in cache
*Normal buffers, 512 bytes (total 641, permanent 512):
*     385 in free list (128 min, 1024 max allowed)
*     469 hits, 43 misses, 0 trims, 129 created
*     0 failures (0 no memory)
 
	To fully support scatter gather on Predator, there are two needs
	for public pools of particles. These two pools fill these needs.

	The first pool is full of small particles. These particles reside in
	very fast SRAM. They are used when a packets outgoing encapsulation
	is too large to fit over the inbound encapsulation. Examples of
	this are TR/FDDI with RIF and ISL. In these cases, a new particle
	needs to be allocated and enqueued onto the front of the packet and the
	outbound encapsulation is written into this particle. Note, if this
	particle pool gets exhausted, the allocation mechanism will try and
	allocate a particle from the normal particle pool which is resident
	in DRAM.

	The second need is to provide a fallback particle pool for
	interfaces that exhaust their private recieve particle pool to
	accomodate bursts of traffic. The Normal particle pool is used
	for this purpose.

*Private particle pools:
*FastEthernet0/0 buffers, 512 bytes (total 400, permanent 400):
*     0 in free list (0 min, 400 max allowed)
*     400 hits, 0 fallbacks
*     400 max cache size, 272 in cache
*     14 buffer threshold, 0 threshold transitions
*Fddi1/0 buffers, 512 bytes (total 384, permanent 384):
*     0 in free list (0 min, 384 max allowed)
*     384 hits, 0 fallbacks
*     384 max cache size, 224 in cache
*     14 buffer threshold, 0 threshold transitions
*Serial5/0 buffers, 512 bytes (total 192, permanent 192):
*     0 in free list (0 min, 192 max allowed)
*     192 hits, 0 fallbacks
*     192 max cache size, 192 in cache
*Ethernet6/0 buffers, 512 bytes (total 128, permanent 128):
*     0 in free list (0 min, 128 max allowed)
*     128 hits, 0 fallbacks
*     128 max cache size, 64 in cache

	Each interface has a private recieve particle pool. The fast interfaces
	try hard to allocate this in SRAM (FDDI, fastether). If there is no
	SRAM, DRAM is used. The slower interfaces get allocated in DRAM.
	Also, the fast interfaces implement a throttling mechanism based on
	the availability of particles in the private pool and consequently
	they do not fallback into the normal particle pool. The slower
	interfaces do not throttle off of the available resource in the
	private pool and do fallback into the normal particle pool.

[12] Predator Memory Map
=======================

	This is not for the faint of hearted. The Predator memory map
	is very complex (and actually best understood thru a mixture
	of reading the source code and looking at the output of "sh memory",
	"sh region" etc on the box itself).

	There are two basic perspectives on the memory map. The CPU's
	view and the port adaptors view. The CPU's view is more complex
	as it has a concept of virtual and physical addresses, so we will
	start there. We will also assume that it is a 32mbyte DRAM box, with
	10 mbytes of DRAM allocated as IO memory, and 1 mbyte of installed
	packet SRAM. Note, there is one part of the memory map that is
	not shown here as it is not used in the software. The CPU can access
	packet SRAM directly, or it can go thru the gt64010 across one
	of the two PCI buses to get at SRAM. This address space is not shown
	as it is not used and not programmed in the virtual address space
	of the CPU.

	Memory			CPU Physical	CPU Virtual 	Attributes
	----------------------------------------------------------------------

	Non Packet DRAM      ---0x00000000   ---0x60000000  	Cached,
			    |     	    |     	    	Write back,
			     ---0x015fffff   ---0x615fffff	L2 cached

	IO DRAM (packet)     ---0x01600000   ---0x01600000 	Uncached
			    |	    	    |
		 	     ---0x01ffffff   ---0x01ffffff

					     ---0x61600000	Cached, 
					    |	      	 	Write back,
					     ---0x61ffffff	L2 cached

			     -0x2,01600000   ---0x31600000 	Cached, 
			    |	    	    | 	   		Write thru,
			     -0x2,01ffffff   ---0x31ffffff	No L2 cache

	IO SRAM (packet)     ---0x4b000000   ---0x4b000000 	Uncached
	Direct access	    |     	    |     
	by CPU.		     ---0x4b0fffff   ---0x4b0fffff

			     -0x2,4b000000   ---0x7b000000 	Cached,
			    |		    |			Write thru,
			     -0x2,4b0fffff   ---0x7b0fffff	No L2 cache

	----------------------------------------------------------------------

	And from the port adaptors perspective, the following is the
	memory map. Note, although the PA's have access to all resources, 
	only the parts of memory that they actually access are shown. Also,
	all memory from a PA's perspective has two mappings. One whereby the
	data comes out of memory with no byte swap IE big endian data
	comes out in big endian format. And the other whereby the data
	comes out swapped IE big endian data comes out in little endian format.

	Memory			PA (no byte swap)	PA (byte swap)
	----------------------------------------------------------------------

	IO DRAM (packet)     ---0x01600000  	    ---0xC1600000
			    |	    	    	   |	
		 	     ---0x01ffffff   	    ---0xC1ffffff

	IO SRAM (packet)     ---0x4b000000    	    ---0x4b800000
			    |			   |
			     ---0x4b0fffff          ---0x4b8fffff

	----------------------------------------------------------------------

	From the above, it can been seen that, for big endian port adaptors,
	the CPU's uncached virtual address for any byte of shared memory
	is identical to the port adaptors. So, the software need only
	pass its address to the device (via register, descriptor or whatever).
	No translation is required for DRAM or SRAM resources.

	Below has the outputs of some usefull memory commands :

pred-ios#sh region
Region Manager:
 
      Start         End     Size(b)  Class  Media  Name
 0x01600000  0x01FFFFFF    10485760  Iomem  R/W    iomem
 0x31600000  0x31FFFFFF    10485760  Iomem  R/W    iomem:(iomem_cwt)
 0x4B000000  0x4B0FFFFF     1048576  PCI    R/W    pcimem
 0x60000000  0x615FFFFF    23068672  Local  R/W    main
 0x600108A0  0x605D1F2F     6035088  IText  R/O    main:text
 0x605D4000  0x607C78BF     2046144  IData  R/W    main:data
 0x607C78C0  0x60820D6F      365744  IBss   R/W    main:bss
 0x60820D70  0x615FFFFF    14545552  Local  R/W    main:heap
 0x7B000000  0x7B0FFFFF     1048576  PCI    R/W    pcimem:(pcimem_cwt)
 0x80000000  0x815FFFFF    23068672  Local  R/W    main:(main_k0)
 0xA0000000  0xA15FFFFF    23068672  Local  R/W    main:(main_k1)
 
pred-ios#sh mem
               Head   Total(b)    Used(b)    Free(b)  Lowest(b) Largest(b)
Processor  60820D70   14545552    3631708   10913844   10724356   10783900
      I/O   1600000   10485760    2139328    8346432    8171184    8302192
      PCI  4B000000    1048576     833428     215148     215148     211892
 
rommon 4 > meminfo
 
Main memory size: 32 MB.
Packet memory size: 1 MB
NVRAM size: 0x20000

Physical Address  : 
ROM               : 0x1fc00000
NVRAM             : 0x1e000000
Boot Flash        : 0x1a000000
Packet SRAM       : 0x4b000000
IO Registers      : 0x1e840200
MP Registers      : 0x1e800000
GT64010 Registers : 0x14000000
Bay 0, IO Card    : 0x48000000
Bay 1, PA 1       : 0x48800000
Bay 2, PA 2       : 0x4d000000
Bay 3, PA 3       : 0x49000000
Bay 4, PA 4       : 0x4d800000
Bay 5, PA 5       : 0x49800000
Bay 6, PA 6       : 0x4e000000

*******************************************************************************
